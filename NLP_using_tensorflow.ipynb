{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_using_tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQCa09JwdfWvLispf3Rcov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apptrixie/NLP__using_tensorflow/blob/main/NLP_using_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNKInMuh4YIh"
      },
      "source": [
        "# Natural language processing \r\n",
        "\r\n",
        "## <u>Tokenization</u>\r\n",
        "In this we try to input sentences and create indexed tokens for each word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWxINtX65Y6F"
      },
      "source": [
        "#importing libraries \r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlpU2cLX5r44"
      },
      "source": [
        "#define the list of sentences\r\n",
        "sentences = [\r\n",
        "             'I love my dog',\r\n",
        "             'I love my cat'\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJwi-3lC54dK",
        "outputId": "940bb50e-8270-4f39-9c27-ef3629193f9c"
      },
      "source": [
        "#creating an instance of the tokenizer object\r\n",
        "#num_words parameter is the maximum no.of words ot keep\r\n",
        "tokenizer = Tokenizer(num_words = 100)\r\n",
        "\r\n",
        "#to get the most frequent 100 words used in sentences\r\n",
        "#the tokenizer goes through all the tokens or words and fits itself to most frequent used\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "\r\n",
        "#the list of words is available as the index\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o7hKK2y7itM",
        "outputId": "ff43318b-8c69-4467-de73-1df0e4acf34a"
      },
      "source": [
        "#the tokenizer is smart enough to not notice the symbol following the word and create a new token for the same \r\n",
        "#for eg:\r\n",
        "sentences = [\r\n",
        "             'I love my dog',\r\n",
        "             'I love my cat',\r\n",
        "             'You love my dog!'\r\n",
        "]\r\n",
        "tokenizer = Tokenizer(num_words = 100)\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKkd9PYH8OnJ"
      },
      "source": [
        "## <u>Turning sentences into data</u>\r\n",
        "A sentence is a sequence of tokens. So, we try to form sequences for a given sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip26mj-f80UY",
        "outputId": "8c16d5a2-f1ec-4624-a613-faad3e08b150"
      },
      "source": [
        "#a sentence is a sequence of indexed tokens\r\n",
        "#adding list of sentences has sentences of various no. of words\r\n",
        "sentences = [\r\n",
        "             'I love my dog',\r\n",
        "             'I love my cat',\r\n",
        "             'You love my dog!',\r\n",
        "             'Do you think my dog is amazing?'\r\n",
        "]\r\n",
        "tokenizer = Tokenizer(num_words = 100)\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print(word_index)\r\n",
        "print()\r\n",
        "\r\n",
        "#the text_to_sequences method converts the tokens to a given sentence sequence \r\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\r\n",
        "print(sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
            "\n",
            "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3Z3CkXI-if2",
        "outputId": "d658de4a-0699-4ba5-db47-692ca504162b"
      },
      "source": [
        "#let's create a test sentence sequence\r\n",
        "test_data = [\r\n",
        "             'i really love my dog',\r\n",
        "             'my dog loves me too'\r\n",
        "]\r\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\r\n",
        "print(test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2, 1, 3], [1, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQFeY-gH_nj6"
      },
      "source": [
        "Here, we see that the words 'really', 'loves', 'me', 'too' do not belong to the training sentences list. The tokens only exist for the words in the training corpus. Only the words in the corpus are used to build a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obUjDko1AdIz",
        "outputId": "5645ce49-8f2a-4aa0-d0c0-30a73043bb32"
      },
      "source": [
        "#now we tend to lose the words that are not in the training corpus and also lose on length of the sentence sequence\r\n",
        "#to overcome this, use the parameter oov_token and \r\n",
        "#set it to a string that it will not encounter anywhere in the test/training corpus\r\n",
        "sentences = [\r\n",
        "             'I love my dog',\r\n",
        "             'I love my cat',\r\n",
        "             'You love my dog!',\r\n",
        "             'Do you think my dog is amazing?'\r\n",
        "]\r\n",
        "\r\n",
        "#oov is 'out of vocabulary' token\r\n",
        "tokenizer = Tokenizer(num_words= 100, oov_token= \"<OOV>\")\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print(\"The word index for training are:\")\r\n",
        "print(word_index)\r\n",
        "print()\r\n",
        "\r\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\r\n",
        "print(\"The word sequences for training set are:\")\r\n",
        "print(sequences)\r\n",
        "print()\r\n",
        "\r\n",
        "test_data = [\r\n",
        "             'i really love my dog',\r\n",
        "             'my dog loves me too'\r\n",
        "]\r\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\r\n",
        "print(\"The word sequences for test set are: \")\r\n",
        "print(test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The word index for training are:\n",
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "The word sequences for training set are:\n",
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "\n",
            "The word sequences for test set are: \n",
            "[[5, 1, 3, 2, 4], [2, 4, 1, 1, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27BnaEPaBw6Y"
      },
      "source": [
        "Here, the length of the sentence sequence is preserved as everytime a new word is encountered in test set it is replaced by < OOV > in the sequence. But, still we have lost some meaning.\r\n",
        "Also, when oov replaces a word in sequence, it can form the same sequence for different sentences.\r\n",
        "For eg: \"my dog loves me too\" is [2, 4, 1, 1, 1]\r\n",
        "and \"my dog loves her too\" is [2, 4, 1, 1, 1]\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4deC4mSEz5a"
      },
      "source": [
        "To create sequences of uniform length we use padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BklEuQ2EPod"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2sT0J90Ebym",
        "outputId": "5fecdd41-5458-4625-e881-5699e257f58d"
      },
      "source": [
        "sentences = [\r\n",
        "             'I love my dog',\r\n",
        "             'I love my cat',\r\n",
        "             'You love my dog!',\r\n",
        "             'Do you think my dog is amazing?'\r\n",
        "]\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words= 100, oov_token= \"<OOV>\")\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print(\"The word index for training are:\")\r\n",
        "print(word_index)\r\n",
        "print()\r\n",
        "\r\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\r\n",
        "print(\"The word sequences for training set are:\")\r\n",
        "print(sequences)\r\n",
        "print()\r\n",
        "\r\n",
        "#all the sentences are set to length of the longest sequence\r\n",
        "print(\"The padded sequence is:\")\r\n",
        "padded = pad_sequences(sequences)\r\n",
        "print(padded)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The word index for training are:\n",
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "The word sequences for training set are:\n",
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "\n",
            "The padded sequence is:\n",
            "[[ 0  0  0  5  3  2  4]\n",
            " [ 0  0  0  5  3  2  7]\n",
            " [ 0  0  0  6  3  2  4]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StarNIlkF0L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86997ab-e8dc-4cba-cb9e-99f99da84839"
      },
      "source": [
        "#if padding after the sentence ends\r\n",
        "print('Padding after the sentence:')\r\n",
        "padded = pad_sequences(sequences, padding= 'post')\r\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padding after the sentence:\n",
            "[[ 5  3  2  4  0  0  0]\n",
            " [ 5  3  2  7  0  0  0]\n",
            " [ 6  3  2  4  0  0  0]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2D2BbQ_Gxqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c8901e-8d37-4c6d-919e-cd772bed6d59"
      },
      "source": [
        "#to specify the maximum length of the padded sequences\r\n",
        "print('Padding after the sentence and the max length of sequences is 5:')\r\n",
        "padded = pad_sequences(sequences, padding= 'post',maxlen=5)\r\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padding after the sentence and the max length of sequences is 5:\n",
            "[[ 5  3  2  4  0]\n",
            " [ 5  3  2  7  0]\n",
            " [ 6  3  2  4  0]\n",
            " [ 9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM3-19S7IaCy"
      },
      "source": [
        "Here, we see that the last sequence in the list shows the last five elements and all other sequences show the first five sequences. The sequences are by default in pre truncate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qk_vx-YHPG3",
        "outputId": "e886952f-1faf-47c5-8d00-accae30d3989"
      },
      "source": [
        "#if sentence are longer than the specified maxlen the you truncate (either post or pre truncate)\r\n",
        "print('Padding after the sentence and the max length of sequences is 5, post-truncating:')\r\n",
        "padded = pad_sequences(sequences, padding= 'post',maxlen=5, truncating='post')\r\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padding after the sentence and the max length of sequences is 5, post-truncating:\n",
            "[[5 3 2 4 0]\n",
            " [5 3 2 7 0]\n",
            " [6 3 2 4 0]\n",
            " [8 6 9 2 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujg1WKa_Hmph",
        "outputId": "5efc0d9c-fc8b-4c91-9df8-c6707e1765e2"
      },
      "source": [
        "#pre truncate\r\n",
        "print('Padding after the sentence and the max length of sequences is 5, pre-truncating:')\r\n",
        "padded = pad_sequences(sequences, padding= 'post',maxlen=5, truncating='pre')\r\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padding after the sentence and the max length of sequences is 5, pre-truncating:\n",
            "[[ 5  3  2  4  0]\n",
            " [ 5  3  2  7  0]\n",
            " [ 6  3  2  4  0]\n",
            " [ 9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZojpNePNJDxh"
      },
      "source": [
        "## <u>Training a model to recognize sentiment in text</u>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_c8zN6LPO09"
      },
      "source": [
        "After completing the pre-processing, we now try and build a classifier to recognize the sentiments in a given text.    \r\n",
        "\r\n",
        "We use dataset of news headlines, where the headlines have been classified into being sarcastic or not. \r\n",
        "We build classifier on this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQxChd-yYYmh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "33bed63e-2170-40f9-e97e-ff27e32fee1d"
      },
      "source": [
        "#importing the  json library to read a json file\r\n",
        "import json\r\n",
        "\r\n",
        "#loading the sarcasm.json file using the json library\r\n",
        "with open(\"Sarcasm_Headlines_Dataset_v2.json\", 'r') as f:\r\n",
        "    datastore = json.load(f)\r\n",
        "\r\n",
        "#creating lists for headlines, whether of not it is sarcastic and the link to the article\r\n",
        "sentences = []\r\n",
        "labels = []\r\n",
        "urls = []\r\n",
        "\r\n",
        "#adding items to the list\r\n",
        "for item in datastore:\r\n",
        "    sentences.append(item['headline'])\r\n",
        "    labels.append(item['is_sarcastic'])\r\n",
        "    urls.append(item['article_link'])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c7bc42999620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#loading the sarcasm.json file using the json library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sarcasm_Headlines_Dataset_v2.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdatastore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Sarcasm_Headlines_Dataset_v2.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whQRUB_OTx0C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}